[![License: MIT](https://img.shields.io/github/license/DarkByte-XI/Job-Market?label=license&style=for-the-badge)
![Made with Python](https://img.shields.io/badge/Python-3.12-blue?flat&logo=python&logoColor=yellow&style=for-the-badge)
![Streamlit](https://img.shields.io/badge/-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![Airflow](https://img.shields.io/badge/Airflow-3.0+-blue?logo=apacheairflow&logoColor=red&style=for-the-badge)
![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)
![Dockerized](https://img.shields.io/badge/Docker-ready-green?logo=docker&style=for-the-badge)
![Maintenance](https://img.shields.io/maintenance/maintained/2025?style=for-the-badge)
![Grafana](https://img.shields.io/badge/Grafana-Dashboard-orange?style=for-the-badge&logo=grafana)

<p align="center">
  <img src="docs/screenshots/job_market_background.jpeg" alt="Job Market Banner" width="970" height="302"/>
</p>

# Job Market

Centralisation et recommandation d‚Äôoffres d‚Äôemploi multicanal.

Ce projet vise √† agr√©ger, nettoyer et proposer des offres d‚Äôemploi issues de plusieurs sources externes (France Travail, Adzuna, JSearch) et √† les exposer via une API de recherche/recommandation performante, utilisable en usage interne ou pour prototypage de projets data RH.

---

## Sommaire

- [Pr√©sentation](#pr√©sentation)
- [Architecture g√©n√©rale](#architecture-g√©n√©rale)
- [Pr√©requis](#pr√©requis)
- [Installation & Ex√©cution](#installation--ex√©cution)
  - [Installation des pr√©requis](#installation-des-pr√©requis)
- [R√©cup√©ration des acc√®s](#r√©cup√©ration-des-acc√®s)
- [Configuration et variables d'environnement](#configuration-et-variables-denvironnement)
- [Permissions et fichiers critiques](#permissions-et-fichiers-critiques)
- [Lancement](#lancement) 
    - [Premi√®re ex√©cution manuelle](#premi√®re-ex√©cution-manuelle)
    - [Lancer Docker Compose](#lancer-docker-compose)
    - [Airflow](#airflow)
- [Pipeline ETL](#pipeline-etl)
    - [1. Extraction](#1-extraction)
    - [2. Transformation et normalisation](#2-transformation-et-normalisation)
    - [3. Chargement](#3-chargement)
- [Orchestration dans Airflow](#orchestration-dans-airflow)
- [API FastAPI](#api-fastapi)
    - [Endpoints](#endpoints)
    - [Exemples d‚Äôutilisation](#exemples-utilisation)
- [Moteur de recommandation](#moteur-de-recommandation)
- [Streamlit](#streamlit)
- [Grafana](#grafana)
  - [Configuration](#configuration)
  - [Importation du dashboard](#importation-du-dashboard)
- [Ressources et dictionnaires](#ressources-et-dictionnaires)
- [Auteurs](#auteurs)

---

## Pr√©sentation

Job Market est une plateforme compl√®te permettant de :

* Collecter des offres d'emploi via plusieurs _**APIs**_ (France Travail, Adzuna, JSearch)
* Nettoyer, enrichir et structurer les donn√©es via un pipeline _**ETL**_
* Proposer un moteur de recommandation via une API _**FastAPI**_ performante
* Visualiser les r√©sultats via une interface _**Streamlit**_
* Orchestrer le tout avec _**Apache Airflow**_, et _**Docker Compose**_

---

## Architecture g√©n√©rale

* ETL Python : Extraction ‚Üí Transformation ‚Üí Chargement (optionnel en PostgreSQL)
* API FastAPI : Endpoints de recherche et consultation d‚Äôentreprises
* Interface Streamlit : Simulation d‚Äôun site d‚Äôemploi
* Base PostgreSQL : Stockage relationnel optimis√© (triggers, vues)
* Orchestration Airflow : D√©clenchement des flux via DAGs
* Docker : Conteneurisation et configuration compl√®te avec docker-compose
* Monitoring (optionnel) : Int√©gration possible avec Prometheus & Grafana

---

## Pr√©requis

* Python 3.10 ou sup√©rieur (recommand√©)
* Docker & Docker Compose
* Homebrew
* Git
* Ports expos√©s :
  * API: ```8000```
  * Streamlit: ```8501```
  * Airflow Webserver: ```8080```

> Toute instance locale utilisant d√©j√† l'un de ces ports devra √™tre arr√™t√©e, 
> ou alors les ports devront √™tre modifi√©s afin d‚Äô√©viter les conflits.
>>La modification peut se faire directement dans le docker-compose.yaml
---


## Installation & Ex√©cution
### Installation des pr√©requis

### **Homebrew (macOS / Linux)**
Homebrew est un gestionnaire de paquets indispensable pour installer facilement
des outils comme git, python, ou docker.
```bash
brew --version
```
Installation (si non install√©) :

```bash
/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"
```

Une fois install√©, ajouter Homebrew au PATH (si ce n‚Äôest pas fait automatiquement) :

#### **macOS (zsh) :**
```bash
echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' >> ~/.zprofile
eval \"$(/opt/homebrew/bin/brew shellenv)\"
```

### Installation de Git
Git doit √™tre install√© pour r√©cup√©rer le d√©p√¥t du projet :

```bash
git --version
```
Si Git n'est pas install√©, sur macOS (Homebrew) :
```bash
brew install git
```

#### **Debian/Ubuntu :**

```bash
sudo apt install git
```

#### **Sur Windows :**
üëâ [T√©l√©charger Git pour Windows](https://git-scm.com/downloads/win)

### Installation de python

Installation rapide :

#### **macOS (Homebrew) :**

```bash
brew install python
```

#### **Debian/Ubuntu :**

```bash
sudo apt update
sudo apt install python3 python3-venv python3-pip
```

#### **Windows :**
üëâ [T√©l√©charger Python (>= 3.10) de pr√©f√©rence 3.12](https://www.python.org/downloads/windows/)

> ‚ö†Ô∏è Important : S'assurer que Python est bien ajout√© au PATH (option d'installation par d√©faut recommand√©e).

#### Une fois tous ces pr√©requis sont install√©s, on peut r√©cup√©rer le projet et proc√©der au lancement :

1. Cloner le d√©p√¥t
```bash
git clone <https://github.com/DarkByte-XI/Job-Market.git>
cd Job_Market
```

2. Cr√©er et activer un environnement virtuel :
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows
```
> üîç Sur Pycharm, cr√©er un projet vient √† cr√©er l'environnement automatiquement.

3. Installer les d√©pendances :
```bash
pip install -r requirements.txt
```

---

## R√©cup√©ration des acc√®s

Pour r√©cup√©rer les acc√®s des API, il est n√©cessaire de cr√©er un compte sur chacun des sites suivants :
* https://developer.adzuna.com/
  * Les acc√®s sont disponibles dans **Dashboard > API Access Details**
* https://francetravail.io/
  * Cr√©er une application et r√©cup√©rer les acc√®s
* https://rapidapi.com/
  * Pour acc√©der aux acc√®s de Jsearch une fois le compte cr√©√© :
  1. https://rapidapi.com/letscrape-6bRBa3QguO5/api/jsearch
  2. Cliquer sur `Job Search` dans `Endpoints` √† gauche de l'√©cran
  3. L'`url`, `x-rapidapi-key` et `x-rapidapi-host` sont disponibles dans le code snippets √† droite de l'√©cran.

--- 

## Configuration et variables d'environnement

Pour s√©curiser et centraliser la configuration sensible (identifiants d‚ÄôAPI, cl√©s secr√®tes, etc.), le projet utilise un fichier `.env` **non versionn√©**.

- Un mod√®le de configuration est fourni‚ÄØ:  
  **`.env_copy`**  
  > Ce fichier contient toutes les variables attendues, mais sans valeur (ou avec des valeurs d‚Äôexemple).
Cette partie est importante pour initier les connexions avec les 
API et se connecter √† la base de donn√©es
### Utilisation

1. **Copier le mod√®le** dans √† la racine :
    ```bash
    cp .env_copy .env
    ```

2. **Compl√®ter** le fichier `.env` avec ses propres identifiants :
    - Cl√©s d‚ÄôAPI pour France Travail, Adzuna, JSearch, etc.
    - Les acc√®s pour Grafana (au choix)
    - Les acc√®s des bases de donn√©es (au choix)
    - G√©n√©rer FERNET_KEY (Airflow) :
    ```bash
    python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
    ```
    - G√©n√©rer INTERNAL_API_SECRET_KEY (Airflow) :
    ```bash
   openssl rand -hex 16
    ``` 

> Certaines valeurs sont pr√©-remplies dans `.env`. 
>> ‚ö†Ô∏èNe pas changer le nom des variables d'environnements !

3. **Ne jamais partager son fichier `.env`**  
   Il contient des informations confidentielles (identifiants personnels, tokens‚Ä¶).

### Bonnes pratiques

- Le fichier `.env` est ignor√© par Git gr√¢ce √† `.gitignore`.
- Ne jamais commiter de cl√©s r√©elles dans le repo !
- Les variables d'environnement sont inject√©es dans les environnements des services dans le docker-compose,
toute modification peut g√©n√©rer des erreurs. √Ä modifier uniquement en cas de n√©cessit√© et de connaissance de l'environnement.

---

## Permissions et fichiers critiques

Avant tout lancement (notamment sous Linux/macOS), il faut s'assurer que :

* `entrypoint.sh` est ex√©cutable pour ex√©cuter automatiquement les fichiers `.sql` :

```bash
chmod +x ./src/sql/entrypoint.sh
````
* Assurer les droits d'√©criture pour Airflow et services associ√©s
```bash
chmod -R u+rwX dags logs plugins data
```

* Si n√©cessaire, appliquer les droits pour Docker
```bash
chmod -R u+rwX ./src
```


## Lancement

### Premi√®re ex√©cution manuelle
Avant de lancer l‚ÄôAPI pour la toute premi√®re fois, il est n√©cessaire d'ex√©cuter le pipeline 
ETL au moins une fois pour alimenter la base de donn√©es et rendre l'API fonctionnelle.

Depuis la racine du projet, ex√©cuter :

```bash
PYTHONPATH=src python ./src/pipelines/main.py
```

Cette commande va :

* Extraire les donn√©es initiales via les APIs externes (France Travail, Adzuna, JSearch).
* Transformer et normaliser ces donn√©es.
* Charger les donn√©es enrichies en base ou en JSON.
* Rendre l'application web Streamlit exploitable

> ‚ö†Ô∏è Sans cette √©tape initiale, l‚ÄôAPI d√©marrera sans donn√©es exploitables.


### **Lancer Docker Compose**
```bash
docker-compose up --build
```
> ‚ö†Ô∏è Avant de lancer le projet, Docker doit √™tre install√© sur votre machine.
> Voici les deux approches principales, adapt√©es √† tous les profils (d√©butant comme avanc√©).

_**Option 1**_ : Docker Desktop (recommand√©, tout-en-un)
Docker Desktop embarque √† la fois l‚Äôinterface graphique, le moteur Docker, Docker Compose, ainsi que tous les outils CLI n√©cessaires (Windows, macOS, Linux).

T√©l√©charger Docker Desktop
üëâ https://www.docker.com/products/docker-desktop/

Suivez les instructions d‚Äôinstallation, puis lancez l‚Äôapplication.

Ouvrez un terminal et v√©rifiez la disponibilit√© :
```bash
docker --version
docker compose version
```

> Remarque : Sous Linux, Docker Desktop n‚Äôest plus obligatoire depuis 2022, 
> mais il offre une exp√©rience unifi√©e.

_**Option 2**_ : Installation CLI uniquement (pour utilisateurs avanc√©s)
Sous macOS (Homebrew)
```bash
brew install --cask docker       # Installe Docker Desktop (GUI et CLI)
# ou pour installer uniquement le CLI Docker :
brew install docker docker-compose
```
> Pour utiliser Docker Desktop, lancez-le depuis le dossier Applications.

_**Sous Linux**_
Utilisez le script officiel d‚Äôinstallation (compatible Ubuntu, Debian, Fedora, etc.) :

```bash
curl -fsSL https://get.docker.com | sudo sh
sudo usermod -aG docker $USER  # Ajoute votre utilisateur au groupe docker
```
D√©connexion puis reconnexion pour appliquer les droits.
Une fois Docker install√©, il faut relancer le build pour que tous les services soient disponibles.

### Airflow

Le projet embarque les services Apache Airflow conteneuris√©s. Ils permettent de planifier les flux ETL de mani√®re robuste et
les ex√©cuter de mani√®re fiable.
Les services airflow permettent de :
* Cr√©er les d√©pendances n√©cessaires et les r√©pertoires essentiels (./airflow/{dags, logs, plugins})
* Initialiser la DB Airflow avec PostgreSQL
* Cr√©er les utilisateurs par d√©faut (admin:admin)
> Les acc√®s √† l'interface de Airflow sont disponibles dans les logs du service
> airflow-apiserver. Pour y acc√©der dans l'application Docker, **containers > airflow-apiserver > Logs**.
> Sinon dans le terminal de l'environnement, √† la racine, √©crire la commande suivante :
```bash
docker logs job_market-airflow-apiserver-1 2>&1 | grep -m 1 "Simple auth manager | Password for user 'admin':"
```
```bash
Sortie attendue :
Simple auth manager | Password for user 'admin': random_key
```
* Initialiser l'apiserver permettant d'acc√©der √† l'interface utilisateur
* Initialiser le dag processor permettant de traiter les dag et les loguer dans la base de donn√©es airflow.
* Initialiser le triggerer permettant de d√©clencher le dag principale : **etl.py** dans ./airflow/dags
* Initialiser le scheduler permettant de planifier le dag.

### Acc√®s √† l‚Äôinterface Airflow :
> http://localhost:8080

---

## Pipeline ETL

### 1. Extraction
Les modules dans ./src/fetch_functions/ d√©finissent les fonctions d'extraction.
Les modules dans src/pipelines/extract.py appellent ces fonctions et orchestrent l'extraction
ainsi que la sauvegarde des sorties dans ./data/raw_data/{source}/.json

### 2. Transformation et normalisation
Le module ./src/pipelines/transform.py :
- Nettoie les donn√©es (accents, minuscules, valeurs parasites, etc)
- Normalise les valeurs
- Harmonise la structure des donn√©es
- Sauvegarde les donn√©es trait√©es dans ./data/processed_data/*.json

### 3. Chargement
Le module ./src/pipelines/load.py :
- R√©cup√®re le dernier fichier transform√© dans ./data/processed_data/
- √âtablie une connexion avec la base de donn√©es lanc√©e dans le docker-compose
- Charge les donn√©es via un processus ThreadPoolExecutor
---

## Orchestration dans Airflow

Le pipeline ETL est orchestr√© via un dag qui se trouve dans le r√©pertoire ./airflow/dags.
Le workflow consiste √† d√©clencher en parall√®le l'extraction des donn√©es des diff√©rentes sources puis transformer
et alimenter la base de donn√©es. Il permet √©galement de mettre √† jour l'API via un rechargement du
dernier fichier extrait.

---

## API FastAPI
L‚ÄôAPI d√©marre en important le moteur de recommandation (TF-IDF, similarit√© cosinus) qui vectorise toutes les offres au d√©marrage‚ÄØ:
**Aucune latence li√©e au chargement des fichiers √† chaque requ√™te.**

### Endpoints
#### 1. /search
Recherche d‚Äôoffres d‚Äôemploi recommand√©es √† partir d‚Äôune requ√™te utilisateur.

**Requ√™te** :
```bash
curl -X GET http://localhost:8000/search?query=data%20engineer
```

**R√©ponse** : liste recommand√©e d‚Äôoffres structur√©es

```bash
[
  {
    "external_id": "5121612668",
    "title": "data engineer hf en alternance",
    "company": "openclassrooms",
    "location": "annecy",
    "code_postal": "74000",
    "salary_min": null,
    "salary_max": null,
    "url": "https://www.adzuna.fr/land/ad/5121612668?..."
  }
]
```

#### 2. /companies
Liste des entreprises distinctes extraites des offres.

**Requ√™te**:
```bash
curl -X GET http://localhost:8000/companies
```

**R√©ponse** : liste d‚Äôentreprises
```bash
[
  {
    "id": "a9f5bb1c9c6e0e9b...",
    "name": "openclassrooms"
  }
]
```
#### 3. /reload
L'endpoint reload permet de recharger l'API avec le dernier fichier extrait via le pipeline
ETL pour maintenir les donn√©es √† jour.
Apr√®s le lancement manuel du pipeline et par la suite l'ex√©cution du docker compose,
l'API r√©cup√®re le fichier transform√© et le recharge automatiquement.

Pour une ex√©cution manuelle:
```bash
curl -X POST http://localhost:8000/reload
```

---

## Moteur de recommandation
D√©fini dans le module recommender.py dans ./src/recommender, il traite le fichier de donn√©es
transform√© √† la suite du pipeline ETL pour afficher une sortie intelligente des offres.
Son mode d'op√©ration d√©pend de la vectorisation des donn√©es et explicitement de la m√©thode de pond√©ration
**TF-IDF** souvent utilis√©e dans la recherche d'informations.
Les r√©sultats sont finalement affich√©s gr√¢ce √† un co√©fficient d√©fini par la **similarit√© cosinus**.
Finalement, les poids de pond√©ration ainsi que le seuil de similarit√© sont d√©finis dans les fonctions de recommandation.

---

## Streamlit
Gr√¢ce √† streamlit qui permet la cr√©ation d'applications web, une interface visuelle pour l'acc√®s
aux donn√©es de l'API a √©t√© configur√©e, permettant ainsi de profiter de la fonctionnalit√© de recherche de mani√®re
interactive.
Streamlit est lanc√©e via docker compose en m√™me temps que les autres services et est disponible
√† l'adresse suivante üëâhttp://localhost:8501

--- 

## Grafana
Grafana est une plateforme de repr√©sentation graphique de donn√©es statistiques open source.
Il est embarqu√© dans les services docker du projet.
Il est disponible √† l'adresse suivante üëâ http://localhost:3000

Pour se connecter, il faut r√©cup√©rer les acc√®s d√©finis √† partir de `.env`.

### Configuration
Une fois √† l'adresse mentionn√©e ci-dessus, les identifiants d'acc√®s se trouvent dans le fichier .env.
Pour cr√©er un dashboard, il faut d'abord √©tablir une connexion avec la base de donn√©es postgres, aliment√©e par les
offres d'emploi.
1. Aller √† l'adresse http://localhost:3000/connections/datasources/new
2. Rechercher PostgreSQL dans la barre de recherche et s√©l√©ctionner.
3. Nommer la connexion ou laisser par d√©faut.
4. Dans la partie connexion, la valeur par d√©faut de l'h√¥te est **localhost:5432**.
5. Fournir le nom de la base de donn√©es, ici **jobs_db**
6. Fournir l'username et le mot de passe, d√©fini dans `.env`.
7. D√©sactiver TLS/SSL et chosir la version 15 de PostgreSQL (similaire √† celle lanc√©e dans docker).
8. Sauvegarder et tester la connexion
> Si la connexion √©choue, v√©rifier que le host, username et mot de passe sont bons.

### Importation du dashboard
√Ä la racine, un fichier JSON, nomm√© `Job_Market_Dashboard-1752856139706.json`, permet d'importer
un dashboard d√©j√† configur√©. Il est possible aussi de cr√©er un dashboard vierge une fois la connexion avec
la base de donn√©e est √©tablie. Des connaissances en SQL sont n√©c√©ssaires pour cr√©er les visualisations.

Pour importer le dashboard :
1. Aller sur http://localhost:3000/dashboards/
2. Cliquer sur `New` ou `Nouveau`
3. `Import` ou `Importer`
4. Copier et coller le contenu du fichier json dans l'espace d√©di√© et `charger`.

---

## Ressources et dictionnaires
### Dossier ressources/ :
- **appellations_code.json** : Codes m√©tiers France Travail, essentiel aux requ√™tes d'extraction.
- **data_appellations.json** : Appariement codes/intitul√©s pour m√©tiers "data".
- **appellations_hightech.json** : Appariement codes/intitul√©s pour m√©tiers de la tech.
- **job_keywords.json** : Mots-cl√©s pour recherches Adzuna et JSearch. 
- **code_pays.json, communes_cp.csv** : fichiers d'enrichissement des localisations.

---

## Auteurs
Projet personnel d√©velopp√© et maintenu par [Dani CHMEIS]() & [Enzo Petrelluzi]().